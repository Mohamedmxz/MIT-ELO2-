{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 â€” Model Selection with Grouped Time-Series Cross-Validation\n",
    "\n",
    "Purpose:\n",
    "- Load the feature-engineered dataset (../4_data_analysis/model_datasets/model_ready_dataset_fe.csv)\n",
    "- Run grouped rolling-origin cross-validation (expanding window per-region), evaluate models across splits\n",
    "- Compare target transforms (raw vs log1p) for Rainfall\n",
    "- Run a small randomized hyperparameter search (custom loop) for RandomForest and XGBoost.\n",
    "- Save best parameters to ../4_data_analysis/model_datasets/best_params.json\n",
    "\n",
    "Notes:\n",
    "- This notebook focuses on safe time-aware validation (no leakage across time or regions).\n",
    "- The CV implemented here builds splits across all regions where each split uses per-region expanding windows and then concatenates indices so the model sees many small series together but never future data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FE data shape: (2040, 21)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from math import sqrt\n",
    "from pprint import pprint\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "fe_path = os.path.join('..','4_data_analysis','model_datasets','model_ready_dataset_fe.csv')\n",
    "if not os.path.exists(fe_path):\n",
    "    raise FileNotFoundError(f'Feature-engineered dataset not found at {fe_path}. Run Notebook 3 first.')\n",
    "df = pd.read_csv(fe_path)\n",
    "print('Loaded FE data shape:', df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['REGION', 'YEAR', 'Month', 'Rainfall', 'Temperature', 'Month_Num', 'Time', 'Rainfall_lag_1', 'Rainfall_lag_2', 'Rainfall_lag_3', 'Rainfall_lag_12', 'Temperature_lag_1', 'Temperature_lag_2', 'Temperature_lag_3', 'Temperature_lag_12', 'Rainfall_roll3', 'Rainfall_roll12', 'Temperature_roll3', 'Temperature_roll12', 'Month_sin', 'Month_cos']\n",
      "Per-region counts:\n",
      "REGION\n",
      "Central    408\n",
      "East       408\n",
      "North      408\n",
      "South      408\n",
      "West       408\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic checks\n",
    "print('Columns:', df.columns.tolist())\n",
    "if 'Time' not in df.columns:\n",
    "    df = df.sort_values(['REGION','YEAR','Month_Num']).reset_index(drop=True)\n",
    "    df['Time'] = df.groupby('REGION').cumcount()\n",
    "\n",
    "print('Per-region counts:')\n",
    "print(df.groupby('REGION').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combined splits: 4\n",
      "Split 0: train 180 rows, val 60 rows\n",
      "Split 1: train 240 rows, val 60 rows\n",
      "Split 2: train 300 rows, val 60 rows\n",
      "Split 3: train 360 rows, val 60 rows\n"
     ]
    }
   ],
   "source": [
    "# Grouped expanding CV builder\n",
    "def grouped_expanding_splits(df, group_col='REGION', time_col='Time',\n",
    "                            initial_window=36, step=6, max_splits=5):\n",
    "    \"\"\"\n",
    "    Returns list of (train_idx, val_idx) where indices are global df indices.\n",
    "    For each region we create expanding windows and then combine splits across regions where all regions have that split available.\n",
    "    \"\"\"\n",
    "    groups = df[group_col].unique()\n",
    "    per_group_splits = {}\n",
    "    for g in groups:\n",
    "        idx = df[df[group_col]==g].sort_values(time_col).index.to_numpy()\n",
    "        n = len(idx)\n",
    "        splits = []\n",
    "        start = initial_window\n",
    "        while start < n and len(splits) < max_splits:\n",
    "            train_idx = idx[:start]\n",
    "            val_idx = idx[start:start+step]\n",
    "            if len(val_idx) == 0:\n",
    "                break\n",
    "            splits.append((train_idx, val_idx))\n",
    "            start += step\n",
    "        per_group_splits[g] = splits\n",
    "\n",
    "    # Combine per-round across groups\n",
    "    combined = []\n",
    "    for round_i in range(max_splits):\n",
    "        train_all = []\n",
    "        val_all = []\n",
    "        for g in groups:\n",
    "            grp_splits = per_group_splits.get(g, [])\n",
    "            if round_i >= len(grp_splits):\n",
    "                train_all = None\n",
    "                break\n",
    "            tr, va = grp_splits[round_i]\n",
    "            train_all.extend(tr.tolist())\n",
    "            val_all.extend(va.tolist())\n",
    "        if train_all is None:\n",
    "            break\n",
    "        combined.append((np.array(sorted(set(train_all))), np.array(sorted(set(val_all)))))\n",
    "    return combined\n",
    "\n",
    "# Example: build splits (adjust initial_window/step as needed for the dataset)\n",
    "splits = grouped_expanding_splits(df, initial_window=36, step=12, max_splits=4)\n",
    "print('Number of combined splits:', len(splits))\n",
    "for i, (tr, va) in enumerate(splits):\n",
    "    print(f'Split {i}: train {len(tr)} rows, val {len(va)} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation helpers\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': rmse(y_true, y_pred),\n",
    "        'mae': mae(y_true, y_pred),\n",
    "        'r2': r2(y_true, y_pred)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test three model types and two target variants.\n",
    "- Models: Ridge (fast baseline), RandomForest (tree), XGBoost \n",
    "- Target variants: Raw Rainfall and log1p(Rainfall)\n",
    "\n",
    "We use a small randomized search (custom loop) rather than GridSearchCV to keep compute limited and to ensure we use our custom CV splits safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate features sample: ['Rainfall_lag_1', 'Rainfall_lag_2', 'Rainfall_lag_3', 'Rainfall_lag_12', 'Temperature_lag_1', 'Temperature_lag_2', 'Temperature_lag_3', 'Temperature_lag_12', 'Rainfall_roll3', 'Rainfall_roll12', 'Temperature_roll3', 'Temperature_roll12', 'Month_sin', 'Month_cos']\n",
      "Modeling features count: 16\n"
     ]
    }
   ],
   "source": [
    "# Feature set for modeling - choose features that exist in FE dataset\n",
    "candidate_features = [c for c in df.columns if c not in ['REGION','YEAR','Month','Month_Num','Rainfall','Temperature','Time']]\n",
    "print('Candidate features sample:', candidate_features[:30])\n",
    "\n",
    "# We'll include YEAR, Time and the created lags/rolls/cyclical if present\n",
    "base_features = [c for c in ['YEAR','Time','Month_sin','Month_cos'] if c in df.columns]\n",
    "lag_features = [c for c in df.columns if c.startswith('Rainfall_lag_') or c.startswith('Temperature_lag_')]\n",
    "roll_features = [c for c in df.columns if c.endswith('_roll3') or c.endswith('_roll12') or c.endswith('_roll3') or c.endswith('_roll12')]\n",
    "\n",
    "features = base_features + lag_features + roll_features\n",
    "features = [f for f in features if f in df.columns]\n",
    "print('Modeling features count:', len(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple param search spaces\n",
    "rf_param_space = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'max_depth': [6, 10, 16, None],\n",
    "    'max_features': [None, 'sqrt', 0.5]\n",
    "}\n",
    "xgb_param_space = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "ridge_param_space = {'alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "# helper to sample one combo from a dict-of-lists\n",
    "def sample_params(space):\n",
    "    return {k: random.choice(v) for k,v in space.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CV evaluation loop (returns average RMSE across splits)\n",
    "def evaluate_model_cv(model_cls, param_space, df, features, target_col, splits, target_transform=None, n_iter=6):\n",
    "    results = []\n",
    "    for it in range(n_iter):\n",
    "        params = sample_params(param_space)\n",
    "        rmses = []\n",
    "        maes = []\n",
    "        r2s = []\n",
    "        for tr_idx, va_idx in splits:\n",
    "            X_tr = df.loc[tr_idx, features]\n",
    "            X_va = df.loc[va_idx, features]\n",
    "            y_tr = df.loc[tr_idx, target_col].values\n",
    "            y_va = df.loc[va_idx, target_col].values\n",
    "            if target_transform is not None:\n",
    "                y_tr = target_transform(y_tr)\n",
    "                \n",
    "            # instantiate model\n",
    "            if model_cls == 'ridge':\n",
    "                model = Ridge(**params)\n",
    "            elif model_cls == 'rf':\n",
    "                model = RandomForestRegressor(n_jobs=-1, **params)\n",
    "            elif model_cls == 'xgb':\n",
    "                try:\n",
    "                    from xgboost import XGBRegressor\n",
    "                except Exception as e:\n",
    "                    raise ImportError('XGBoost not installed or failed to import: ' + str(e))\n",
    "                model = XGBRegressor(objective='reg:squarederror', n_jobs=1, **params)\n",
    "            else:\n",
    "                raise ValueError('Unknown model class')\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_va)\n",
    "            # inverse transform for scoring if transform used\n",
    "            if target_transform is not None:\n",
    "                # for log1p we applied y = log1p(y); inverse is expm1\n",
    "                y_pred_inv = np.expm1(y_pred)\n",
    "                y_va_inv = np.expm1(y_va)\n",
    "            else:\n",
    "                y_pred_inv = y_pred\n",
    "                y_va_inv = y_va\n",
    "            rmses.append(mean_squared_error(y_va_inv, y_pred_inv, squared=False))\n",
    "            maes.append(mean_absolute_error(y_va_inv, y_pred_inv))\n",
    "            r2s.append(r2_score(y_va_inv, y_pred_inv))\n",
    "        results.append({'params': params, 'rmse_mean': np.mean(rmses), 'mae_mean': np.mean(maes), 'r2_mean': np.mean(r2s)})\n",
    "    # return sorted by rmse\n",
    "    results = sorted(results, key=lambda x: x['rmse_mean'])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Ridge on raw target...\n",
      "[{'mae_mean': 0.45230296009969895,\n",
      "  'params': {'alpha': 1.0},\n",
      "  'r2_mean': 0.8497176895038167,\n",
      "  'rmse_mean': 0.6746728295841031},\n",
      " {'mae_mean': 0.45602294830794365,\n",
      "  'params': {'alpha': 0.1},\n",
      "  'r2_mean': 0.8484244632651129,\n",
      "  'rmse_mean': 0.6770691965310607},\n",
      " {'mae_mean': 0.45602294830794365,\n",
      "  'params': {'alpha': 0.1},\n",
      "  'r2_mean': 0.8484244632651129,\n",
      "  'rmse_mean': 0.6770691965310607}]\n",
      "\n",
      "Running Ridge on log1p target...\n",
      "[{'mae_mean': 53.8187593218649,\n",
      "  'params': {'alpha': 10.0},\n",
      "  'r2_mean': -0.03625147664667383,\n",
      "  'rmse_mean': 255.7463811419369},\n",
      " {'mae_mean': 53.83602776169591,\n",
      "  'params': {'alpha': 1.0},\n",
      "  'r2_mean': -0.036637997548354506,\n",
      "  'rmse_mean': 255.77905884334683},\n",
      " {'mae_mean': 53.83602776169591,\n",
      "  'params': {'alpha': 1.0},\n",
      "  'r2_mean': -0.036637997548354506,\n",
      "  'rmse_mean': 255.77905884334683}]\n",
      "\n",
      "Running small RF search on raw target...\n",
      "[{'mae_mean': 0.35895996287680243,\n",
      "  'params': {'max_depth': 10, 'max_features': 0.5, 'n_estimators': 200},\n",
      "  'r2_mean': 0.8602826345366565,\n",
      "  'rmse_mean': 0.65121020868696},\n",
      " {'mae_mean': 0.361880244004038,\n",
      "  'params': {'max_depth': 10, 'max_features': 0.5, 'n_estimators': 400},\n",
      "  'r2_mean': 0.8583946409419592,\n",
      "  'rmse_mean': 0.6550085840767081},\n",
      " {'mae_mean': 0.3680338936678385,\n",
      "  'params': {'max_depth': 6, 'max_features': 0.5, 'n_estimators': 200},\n",
      "  'r2_mean': 0.8552818410189972,\n",
      "  'rmse_mean': 0.6614759245561754}]\n",
      "\n",
      "Running XGBoost search on log1p target (if installed)...\n",
      "[{'mae_mean': 0.3710106973135456,\n",
      "  'params': {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200},\n",
      "  'r2_mean': 0.8387724325789898,\n",
      "  'rmse_mean': 0.7064995072413132},\n",
      " {'mae_mean': 0.3890747448023151,\n",
      "  'params': {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100},\n",
      "  'r2_mean': 0.826694367016419,\n",
      "  'rmse_mean': 0.7289823661437689},\n",
      " {'mae_mean': 0.388774591108474,\n",
      "  'params': {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 100},\n",
      "  'r2_mean': 0.8208826362324839,\n",
      "  'rmse_mean': 0.7415945465232567}]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a few configurations\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "target = 'Rainfall'\n",
    "splits_for_eval = splits  # from earlier\n",
    "\n",
    "print('Running Ridge on raw target...')\n",
    "ridge_results_raw = evaluate_model_cv('ridge', ridge_param_space, df, features, target, splits_for_eval, target_transform=None, n_iter=3)\n",
    "pprint(ridge_results_raw[:3])\n",
    "\n",
    "print('\\nRunning Ridge on log1p target...')\n",
    "ridge_results_log = evaluate_model_cv('ridge', ridge_param_space, df, features, target, splits_for_eval, target_transform=np.log1p, n_iter=3)\n",
    "pprint(ridge_results_log[:3])\n",
    "\n",
    "print('\\nRunning small RF search on raw target...')\n",
    "rf_results_raw = evaluate_model_cv('rf', rf_param_space, df, features, target, splits_for_eval, target_transform=None, n_iter=4)\n",
    "pprint(rf_results_raw[:3])\n",
    "\n",
    "# Try XGBoost if available\n",
    "try:\n",
    "    print('\\nRunning XGBoost search on raw target')\n",
    "    xgb_results = evaluate_model_cv('xgb', xgb_param_space, df, features, target, splits_for_eval, target_transform=None, n_iter=4)\n",
    "    pprint(xgb_results[:3])\n",
    "except ImportError as e:\n",
    "    print('XGBoost not available; skipping XGB evaluation:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best params to ..\\4_data_analysis\\model_datasets\\best_params.json\n",
      "{'rf_raw': {'max_depth': 10, 'max_features': 0.5, 'n_estimators': 200},\n",
      " 'ridge_log1p': {'alpha': 10.0},\n",
      " 'ridge_raw': {'alpha': 1.0},\n",
      " 'xgb_log1p': {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}}\n"
     ]
    }
   ],
   "source": [
    "# Save best found hyperparameters (examples: top of each results list)\n",
    "best_params = {}\n",
    "if ridge_results_raw:\n",
    "    best_params['ridge_raw'] = ridge_results_raw[0]['params']\n",
    "if ridge_results_log:\n",
    "    best_params['ridge_log1p'] = ridge_results_log[0]['params']\n",
    "if rf_results_raw:\n",
    "    best_params['rf_raw'] = rf_results_raw[0]['params']\n",
    "try:\n",
    "    if xgb_results:\n",
    "        best_params['xgb_log1p'] = xgb_results[0]['params']\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "out_params_path = os.path.join('..','4_data_analysis','model_datasets','best_params.json')\n",
    "with open(out_params_path, 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print('Saved best params to', out_params_path)\n",
    "pprint(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes and next steps\n",
    "- This notebook ran a short randomized-style search using grouped expanding-window CV. You can extend n_iter and the param spaces for more exhaustive tuning.\n",
    "- The CV function returns splits that ensure *no future data from any region is used for training* when evaluating validation folds.\n",
    "- The saved best_params.json will be used in Notebook 5 for diagnostics/interpretability and Notebook 6 for final forecasts and model saving.\n",
    "\n",
    "If any cell errors or you want me to change the CV window/step sizes, tell me the typical length of the time series per region and I'll adjust defaults and regenerate the notebook if desired."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
