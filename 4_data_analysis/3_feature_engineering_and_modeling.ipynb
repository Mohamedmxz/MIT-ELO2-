{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 — Feature Engineering and Minimal Modeling (Sanity Check)\n",
    "\n",
    "This notebook creates deterministic features (lags, rolling stats, cyclical month features), logs data-loss due to feature creation, saves a feature-engineered dataset, and runs a minimal sanity-check modeling step (train/test split and quick models).\n",
    "\n",
    "Notes:\n",
    "- Uses the canonical cleaned CSV produced by Notebook 2: ../4_data_analysis/model_datasets/model_ready_dataset_clean.csv\n",
    "- Saves the feature-engineered CSV to ../4_data_analysis/model_datasets/model_ready_dataset_fe.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ridge\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n",
      "File \u001b[1;32mc:\\Users\\amhx1\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\amhx1\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\amhx1\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amhx1\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amhx1\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[1;32mc:\\Users\\amhx1\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:305\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_importlib\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\amhx1\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix, csr_sample_values)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# Imports and paths\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "np.random.seed(42)\n",
    "data_in_paths = [\n",
    "    os.path.join('..','4_data_analysis','model_datasets','model_ready_dataset_fe.csv'),  \n",
    "    os.path.join('..','4_data_analysis','model_datasets','model_ready_dataset_clean.csv'),\n",
    "    os.path.join('..','4_data_analysis','model_datasets','model_ready_dataset.csv'),\n",
    "    os.path.join('..','1_datasets','Final_dataset','final_merged_dataset.csv')\n",
    "]\n",
    "out_dir = os.path.join('..','4_data_analysis','model_datasets')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "fe_out_path = os.path.join(out_dir, 'model_ready_dataset_fe.csv')\n",
    "\n",
    "print('Looking for input files (in order):')\n",
    "pprint(data_in_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ..\\4_data_analysis\\model_datasets\\model_ready_dataset_clean.csv\n",
      "Loaded shape: (2100, 15)\n"
     ]
    }
   ],
   "source": [
    "# Robust loader: prefer FE if present, else cleaned, else model_ready, else wide final_merged\n",
    "loaded = None\n",
    "for p in data_in_paths:\n",
    "    if os.path.exists(p):\n",
    "        print('Loading', p)\n",
    "        loaded = p\n",
    "        break\n",
    "if loaded is None:\n",
    "    # Try a recursive search as a helpful fallback\n",
    "    import glob\n",
    "    matches = glob.glob('**/model_ready_dataset*.csv', recursive=True) + glob.glob('**/final_merged_dataset.csv', recursive=True)\n",
    "    matches = sorted(set(matches))\n",
    "    if matches:\n",
    "        print('Found candidates:', matches)\n",
    "        loaded = matches[0]\n",
    "    else:\n",
    "        raise FileNotFoundError('No input dataset found. Place the cleaned dataset at ../4_data_analysis/model_datasets/model_ready_dataset_clean.csv or the wide file at ../1_datasets/Final_dataset/final_merged_dataset.csv')\n",
    "\n",
    "df = pd.read_csv(loaded)\n",
    "print('Loaded shape:', df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after standardization: ['REGION', 'YEAR', 'Month', 'Rainfall', 'Temperature', 'Month_Num', 'Time', 'Rainfall_lag_1', 'Rainfall_lag_2', 'Rainfall_lag_3', 'Rainfall_lag_12', 'Temperature_lag_1', 'Temperature_lag_2', 'Temperature_lag_3', 'Temperature_lag_12']\n"
     ]
    }
   ],
   "source": [
    "# Ensure YEAR/Month_Num exist and standardize column names\n",
    "def ensure_year_month(df):\n",
    "    if 'Year' in df.columns and 'YEAR' not in df.columns:\n",
    "        df = df.rename(columns={'Year':'YEAR'})\n",
    "    if 'MONTH' in df.columns and 'Month' not in df.columns:\n",
    "        df = df.rename(columns={'MONTH':'Month'})\n",
    "    if 'Month_Num' not in df.columns:\n",
    "        if 'Month' in df.columns and df['Month'].dtype == object:\n",
    "            month_map = {\"JAN\":1,\"FEB\":2,\"MAR\":3,\"APR\":4,\"MAY\":5,\"JUN\":6,\n",
    "                        \"JUL\":7,\"AUG\":8,\"SEP\":9,\"OCT\":10,\"NOV\":11,\"DEC\":12}\n",
    "            df['Month_Num'] = df['Month'].map(month_map)\n",
    "        elif 'Month' in df.columns:\n",
    "            df['Month_Num'] = df['Month']\n",
    "        else:\n",
    "            raise KeyError('No Month or Month_Num column found.')\n",
    "    if 'YEAR' not in df.columns and 'year' in df.columns:\n",
    "        df = df.rename(columns={'year':'YEAR'})\n",
    "    return df\n",
    "\n",
    "def standardize_lag_names(df):\n",
    "    rename_map = {\n",
    "        'Rain_lag_1': 'Rainfall_lag_1', 'Rain_lag_2': 'Rainfall_lag_2', 'Rain_lag_3': 'Rainfall_lag_3', 'Rain_lag_12': 'Rainfall_lag_12',\n",
    "        'Temp_lag_1': 'Temperature_lag_1', 'Temp_lag_2': 'Temperature_lag_2', 'Temp_lag_3': 'Temperature_lag_3', 'Temp_lag_12': 'Temperature_lag_12',\n",
    "    }\n",
    "    present = {k:v for k,v in rename_map.items() if k in df.columns}\n",
    "    if present:\n",
    "        df = df.rename(columns=present)\n",
    "    # if both short and canonical exist, drop the short\n",
    "    for short, canon in rename_map.items():\n",
    "        if short in df.columns and canon in df.columns:\n",
    "            df.drop(columns=[short], inplace=True)\n",
    "    return df\n",
    "\n",
    "df = ensure_year_month(df)\n",
    "df = standardize_lag_names(df)\n",
    "print('Columns after standardization:', df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns now include lags and rolls. Total columns: 21\n"
     ]
    }
   ],
   "source": [
    "# Feature creation settings\n",
    "LAGS = [1,2,3,12]\n",
    "ROLL_WINDOWS = {'roll3':3, 'roll12':12}\n",
    "ROLL_MIN_PERIODS = {'roll3':3, 'roll12':12}  # can change to allow partial windows if desired\n",
    "\n",
    "# Create lag columns if missing\n",
    "for var in ['Rainfall','Temperature']:\n",
    "    for lag in LAGS:\n",
    "        cname = f\"{var}_lag_{lag}\"\n",
    "        if cname not in df.columns:\n",
    "            df[cname] = df.groupby('REGION')[var].shift(lag)\n",
    "            print('Created', cname)\n",
    "\n",
    "# Create rolling features using prior months only (shift(1) before rolling) to avoid leakage\n",
    "for var in ['Rainfall','Temperature']:\n",
    "    for label, window in ROLL_WINDOWS.items():\n",
    "        minp = ROLL_MIN_PERIODS[label]\n",
    "        colname = f\"{var}_{label}\"\n",
    "        if colname not in df.columns:\n",
    "            df[colname] = df.groupby('REGION')[var].transform(lambda x: x.shift(1).rolling(window=window, min_periods=minp).mean())\n",
    "            print('Created', colname)\n",
    "\n",
    "# Cyclical month features\n",
    "df['Month_sin'] = np.sin(2 * np.pi * (df['Month_Num'] / 12))\n",
    "df['Month_cos'] = np.cos(2 * np.pi * (df['Month_Num'] / 12))\n",
    "\n",
    "print('\\nColumns now include lags and rolls. Total columns:', len(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping: 2100\n",
      "Rows with any missing required feature: 60\n",
      "Percentage lost: 2.857142857142857\n",
      "\n",
      "Rows dropped per REGION (top):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "REGION\n",
       "Central    12\n",
       "East       12\n",
       "North      12\n",
       "South      12\n",
       "West       12\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping: 2040\n",
      "Saved feature-engineered dataset to ..\\4_data_analysis\\model_datasets\\model_ready_dataset_fe.csv\n"
     ]
    }
   ],
   "source": [
    "# Report row counts before and after dropping NaNs for required features\n",
    "required_for_model = [f\"Rainfall_lag_{i}\" for i in LAGS] + [f\"Temperature_lag_{i}\" for i in LAGS] + \\\n",
    "                 [f\"Rainfall_{k}\" for k in ROLL_WINDOWS.keys()] + [f\"Temperature_{k}\" for k in ROLL_WINDOWS.keys()] + ['Month_sin','Month_cos','YEAR','Time']\n",
    "required_for_model = [c for c in required_for_model if c in df.columns]\n",
    "\n",
    "print('Number of rows before dropping:', df.shape[0])\n",
    "\n",
    "# Determine exactly what would be dropped by required columns\n",
    "na_mask = df[required_for_model].isna().any(axis=1)\n",
    "n_missing_rows = na_mask.sum()\n",
    "print('Rows with any missing required feature:', n_missing_rows)\n",
    "print('Percentage lost:', 100.0 * n_missing_rows / df.shape[0])\n",
    "\n",
    "# Optionally inspect which regions lose many rows\n",
    "if n_missing_rows > 0:\n",
    "    dropped_by_region = df[na_mask].groupby('REGION').size().sort_values(ascending=False)\n",
    "    print('\\nRows dropped per REGION (top):')\n",
    "    display(dropped_by_region.head(20))\n",
    "\n",
    "# Now drop and save the FE dataset\n",
    "df_fe = df.dropna(subset=required_for_model).reset_index(drop=True)\n",
    "print('Number of rows after dropping:', df_fe.shape[0])\n",
    "\n",
    "df_fe.to_csv(fe_out_path, index=False)\n",
    "print('Saved feature-engineered dataset to', fe_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1920, 21) Test shape: (120, 21)\n"
     ]
    }
   ],
   "source": [
    "# Quick time-aware train/test split per region (sanity check)\n",
    "def train_test_time_split(df, group_col='REGION', time_col='Time', test_periods=24):\n",
    "    train_parts, test_parts = [], []\n",
    "    for name, g in df.groupby(group_col):\n",
    "        g_sorted = g.sort_values(time_col).reset_index(drop=True)\n",
    "        if len(g_sorted) <= test_periods:\n",
    "            raise ValueError(f\"Region {name} has <= {test_periods} rows; reduce test_periods or drop region\")\n",
    "        train_parts.append(g_sorted.iloc[:-test_periods].copy())\n",
    "        test_parts.append(g_sorted.iloc[-test_periods:].copy())\n",
    "    return pd.concat(train_parts).reset_index(drop=True), pd.concat(test_parts).reset_index(drop=True)\n",
    "\n",
    "test_periods = 24\n",
    "train_df, test_df = train_test_time_split(df_fe, group_col='REGION', time_col='Time', test_periods=test_periods)\n",
    "print('Train shape:', train_df.shape, 'Test shape:', test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick features length: 9\n",
      "Ridge RMSE: 0.9151413571029801 MAE: 0.5323600950631743\n",
      "RF RMSE: 0.8627483051848939 MAE: 0.4905724202097507\n",
      "\n",
      "Sanity-check modeling completed. These are baseline results — full CV and tuning are in Notebook 4.\n"
     ]
    }
   ],
   "source": [
    "# Minimal modeling sanity-check: train a Ridge (linear) and RandomForest for Rainfall (no heavy tuning)\n",
    "from math import sqrt\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Choose features for quick test: time + cyclical + lag features\n",
    "quick_features = ['YEAR','Time','Month_sin','Month_cos'] + [f'Rainfall_lag_{i}' for i in LAGS] + [f'Temperature_lag_{i}' for i in [1]]\n",
    "quick_features = [c for c in quick_features if c in train_df.columns]\n",
    "print('Quick features length:', len(quick_features))\n",
    "\n",
    "X_tr = train_df[quick_features]\n",
    "X_te = test_df[quick_features]\n",
    "y_tr = train_df['Rainfall']\n",
    "y_te = test_df['Rainfall']\n",
    "\n",
    "# Ridge\n",
    "lr = Ridge(alpha=1.0, random_state=42)\n",
    "lr.fit(X_tr, y_tr)\n",
    "lr_pred = lr.predict(X_te)\n",
    "print('Ridge RMSE:', rmse(y_te, lr_pred), 'MAE:', mean_absolute_error(y_te, lr_pred))\n",
    "\n",
    "# Random Forest (small)\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_tr, y_tr)\n",
    "rf_pred = rf.predict(X_te)\n",
    "print('RF RMSE:', rmse(y_te, rf_pred), 'MAE:', mean_absolute_error(y_te, rf_pred))\n",
    "\n",
    "print('\\nSanity-check modeling completed. These are baseline results — full CV and tuning are in Notebook 4.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we saved and what to do next\n",
    "- Saved: ../4_data_analysis/model_datasets/model_ready_dataset_fe.csv (feature-engineered dataset with lags, rolls, cyclical month features)\n",
    "- Notebook 4 (next) performs robust model selection using grouped time-series cross-validation and simple hyperparameter search. Run that notebook once this one finishes successfully.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
