{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6 — Final RandomForest Models (Region-aware) and Forecast to 2030\n",
    "\n",
    "This notebook trains two RandomForest models (Rainfall and Temperature) in a region-aware way (REGION one-hot), saves holdout predictions, and then runs a recursive per-region forecasting procedure to produce monthly forecasts up to Dec 2030. The recursive forecasting retrains per-region models on each region's full history and steps forward month-by-month, rebuilding lag/rolling features using actuals and previous predictions.\n",
    "\n",
    "Outputs:\n",
    "- ../4_data_analysis/model_datasets/final_forecasts.csv (holdout predictions)\n",
    "- ../4_data_analysis/model_datasets/per_region_final_metrics_rf.csv\n",
    "- ../4_data_analysis/model_datasets/models/rf_rain.joblib and rf_temp.joblib (global RFs)\n",
    "- ../4_data_analysis/model_datasets/models/rf_rain_<region>.joblib and rf_temp_<region>.joblib (per-region models used for 2030 forecasts)\n",
    "- ../4_data_analysis/model_datasets/final_forecasts_to_2030.csv (recursive forecasts to 2030)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "base_dir = os.path.join('..','4_data_analysis','model_datasets')\n",
    "fe_path = os.path.join(base_dir, 'model_ready_dataset_fe.csv')\n",
    "params_path = os.path.join(base_dir, 'best_params.json')\n",
    "models_dir = os.path.join(base_dir, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FE data shape: (2040, 21)\n",
      "Loaded best params keys: ['ridge_raw', 'ridge_log1p', 'rf_raw', 'xgb_log1p']\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fe_path):\n",
    "    raise FileNotFoundError(f'Feature-engineered dataset not found at {fe_path}')\n",
    "df = pd.read_csv(fe_path)\n",
    "print('Loaded FE data shape:', df.shape)\n",
    "\n",
    "best_params = {}\n",
    "if os.path.exists(params_path):\n",
    "    with open(params_path) as f:\n",
    "        best_params = json.load(f)\n",
    "print('Loaded best params keys:', list(best_params.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-region counts sample:\n",
      "REGION\n",
      "Central    408\n",
      "East       408\n",
      "North      408\n",
      "South      408\n",
      "West       408\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure Time column exists and per-region ordering (same as Notebook 5)\n",
    "if 'Time' not in df.columns:\n",
    "    df = df.sort_values(['REGION','YEAR','Month_Num']).reset_index(drop=True)\n",
    "    df['Time'] = df.groupby('REGION').cumcount()\n",
    "print('Per-region counts sample:')\n",
    "print(df.groupby('REGION').size().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1890, 21) Test shape: (150, 21)\n"
     ]
    }
   ],
   "source": [
    "def train_test_time_split(df, group_col='REGION', time_col='Time', test_periods=30):\n",
    "    train_parts, test_parts = [], []\n",
    "    for name, g in df.groupby(group_col):\n",
    "        g_sorted = g.sort_values(time_col).reset_index(drop=True)\n",
    "        if len(g_sorted) <= test_periods:\n",
    "            raise ValueError(f\"Region {name} has <= {test_periods} rows; reduce test_periods or drop region\")\n",
    "        train_parts.append(g_sorted.iloc[:-test_periods].copy())\n",
    "        test_parts.append(g_sorted.iloc[-test_periods:].copy())\n",
    "    return pd.concat(train_parts).reset_index(drop=True), pd.concat(test_parts).reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_time_split(df, test_periods=30)\n",
    "print('Train shape:', train_df.shape, 'Test shape:', test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode REGION (one-hot) to let the global RFs learn region-specific effects. Build dummies on the concatenated train+test to guarantee consistent columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After get_dummies, train_d shape: (1890, 25) test_d shape: (150, 25)\n"
     ]
    }
   ],
   "source": [
    "# Keep original REGION column for final output\n",
    "test_region_series = test_df['REGION'].reset_index(drop=True)\n",
    "\n",
    "# Concatenate and get dummies for REGION\n",
    "combined = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "combined_d = pd.get_dummies(combined, columns=['REGION'], prefix='REG')\n",
    "\n",
    "train_d = combined_d.iloc[:len(train_df)].reset_index(drop=True).copy()\n",
    "test_d = combined_d.iloc[len(train_df):].reset_index(drop=True).copy()\n",
    "print('After get_dummies, train_d shape:', train_d.shape, 'test_d shape:', test_d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 19\n"
     ]
    }
   ],
   "source": [
    "# Build feature list: exclude identifiers and targets; include REGION dummies\n",
    "exclude = ['YEAR','Month','Month_Num','Rainfall','Temperature','Time']\n",
    "# keep YEAR and Time as features explicitly\n",
    "features = [c for c in train_d.columns if c not in exclude]\n",
    "print('Feature count:', len(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred,)\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "def r2(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred)\n",
    "def metrics(y_true, y_pred):\n",
    "    return {'rmse': rmse(y_true, y_pred), 'mae': mae(y_true, y_pred), 'r2': r2(y_true, y_pred)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF params source: provided\n"
     ]
    }
   ],
   "source": [
    "def rf_from_params(params_source):\n",
    "    params = {} if params_source is None else params_source.copy()\n",
    "    rf_kwargs = {}\n",
    "    rf_kwargs['n_estimators'] = int(params.get('n_estimators', 200))\n",
    "    rf_kwargs['max_depth'] = None if params.get('max_depth') in [None,'None'] else params.get('max_depth')\n",
    "    rf_kwargs['max_features'] = params.get('max_features', 'auto')\n",
    "    rf_kwargs['random_state'] = 42\n",
    "    rf_kwargs['n_jobs'] = -1\n",
    "    return RandomForestRegressor(**rf_kwargs)\n",
    "\n",
    "# choose RF params if present\n",
    "rf_params = None\n",
    "if 'rf_raw' in best_params:\n",
    "    rf_params = best_params['rf_raw']\n",
    "elif 'rf_log1p' in best_params:\n",
    "    rf_params = best_params['rf_log1p']\n",
    "elif 'rf' in best_params:\n",
    "    rf_params = best_params['rf']\n",
    "print('RF params source:', 'provided' if rf_params is not None else 'default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout metrics (global):\n",
      "{'rain': {'mae': 0.5006030466197376,\n",
      "          'r2': 0.8474583208914142,\n",
      "          'rmse': 0.8835635569652258},\n",
      " 'temp': {'mae': 0.8658784768450096,\n",
      "          'r2': 0.9312898684634472,\n",
      "          'rmse': 1.2051059439324308}}\n"
     ]
    }
   ],
   "source": [
    "# Prepare training matrices\n",
    "X_tr = train_d[features]\n",
    "X_te = test_d[features]\n",
    "\n",
    "y_tr_rain = train_d['Rainfall'].values\n",
    "y_te_rain = test_d['Rainfall'].values\n",
    "\n",
    "y_tr_temp = train_d['Temperature'].values\n",
    "y_te_temp = test_d['Temperature'].values\n",
    "\n",
    "# Train RF for Rainfall (global, region-dummy aware)\n",
    "rf_rain = rf_from_params(rf_params)\n",
    "rf_rain.fit(X_tr, y_tr_rain)\n",
    "pred_rain = rf_rain.predict(X_te)\n",
    "pred_rain = np.clip(pred_rain, 0, None)\n",
    "\n",
    "# Train RF for Temperature (global, region-dummy aware)\n",
    "rf_temp = rf_from_params(rf_params)\n",
    "rf_temp.fit(X_tr, y_tr_temp)\n",
    "pred_temp = rf_temp.predict(X_te)\n",
    "\n",
    "results = {\n",
    "    'rain': metrics(y_te_rain, pred_rain),\n",
    "    'temp': metrics(y_te_temp, pred_temp)\n",
    "}\n",
    "print('Holdout metrics (global):')\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final (holdout) forecasts to ..\\4_data_analysis\\model_datasets\\final_forecasts.csv\n"
     ]
    }
   ],
   "source": [
    "# Build final output including REGION and DATE for the holdout predictions\n",
    "test_out = test_df.reset_index(drop=True).copy()\n",
    "test_out['Predicted_Rainfall'] = pred_rain\n",
    "test_out['Predicted_Temperature'] = pred_temp\n",
    "\n",
    "# Build DATE column using YEAR and Month_Num if present, else use Month\n",
    "if 'Month_Num' in test_out.columns:\n",
    "    month_col = 'Month_Num'\n",
    "else:\n",
    "    month_col = 'Month'\n",
    "\n",
    "def build_date(row):\n",
    "    try:\n",
    "        m = int(row[month_col])\n",
    "    except Exception:\n",
    "        try:\n",
    "            m = pd.to_datetime(str(row[month_col]), format='%b').month\n",
    "        except Exception:\n",
    "            try:\n",
    "                m = pd.to_datetime(str(row[month_col]), format='%B').month\n",
    "            except Exception:\n",
    "                m = 1\n",
    "    return pd.Timestamp(year=int(row['YEAR']), month=int(m), day=1)\n",
    "\n",
    "test_out['DATE'] = test_out.apply(build_date, axis=1)\n",
    "\n",
    "final_forecasts = test_out[['DATE','REGION','Predicted_Rainfall','Predicted_Temperature']].copy()\n",
    "final_forecasts = final_forecasts.sort_values(['REGION','DATE']).reset_index(drop=True)\n",
    "\n",
    "out_path = os.path.join(base_dir, 'final_forecasts.csv')\n",
    "final_forecasts.to_csv(out_path, index=False)\n",
    "print('Saved final (holdout) forecasts to', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-region metrics to ..\\4_data_analysis\\model_datasets\\per_region_final_metrics_rf.csv\n"
     ]
    }
   ],
   "source": [
    "# Save per-region metrics for holdout\n",
    "rows = []\n",
    "for region, g in test_out.groupby('REGION'):\n",
    "    rows.append({'REGION': region,\n",
    "                'rain_rmse': rmse(g['Rainfall'], g['Predicted_Rainfall']),\n",
    "                'rain_mae': mae(g['Rainfall'], g['Predicted_Rainfall']),\n",
    "                'rain_r2': r2(g['Rainfall'], g['Predicted_Rainfall']),\n",
    "                'temp_rmse': rmse(g['Temperature'], g['Predicted_Temperature']),\n",
    "                'temp_mae': mae(g['Temperature'], g['Predicted_Temperature']),\n",
    "                'temp_r2': r2(g['Temperature'], g['Predicted_Temperature']),\n",
    "                'n': len(g)})\n",
    "per_region_metrics = pd.DataFrame(rows).sort_values('REGION')\n",
    "per_region_metrics.to_csv(os.path.join(base_dir, 'per_region_final_metrics_rf.csv'), index=False)\n",
    "print('Saved per-region metrics to', os.path.join(base_dir, 'per_region_final_metrics_rf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved RF models to: ..\\4_data_analysis\\model_datasets\\models\\rf_rain.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp.joblib\n"
     ]
    }
   ],
   "source": [
    "# Ensure directory exists\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "rf_rain_path = os.path.join(models_dir, 'rf_rain.joblib')\n",
    "rf_temp_path = os.path.join(models_dir, 'rf_temp.joblib')\n",
    "\n",
    "joblib.dump(rf_rain, rf_rain_path)\n",
    "joblib.dump(rf_temp, rf_temp_path)\n",
    "\n",
    "print('Saved RF models to:', rf_rain_path, rf_temp_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast to 2030 — per-region recursive forecasting.\n",
    "\n",
    "The following cell retrains per-region RandomForest models on each region's full history and performs iterative forecasts month-by-month until Dec 2030. It uses lag/rolling columns heuristically detected from the FE dataset (columns containing 'lag' or 'roll' patterns). The exact code from your requested snippet is integrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base features (kept static): ['Month_sin', 'Month_cos']\n",
      "Rainfall lag cols found: ['Rainfall_lag_1', 'Rainfall_lag_12', 'Rainfall_lag_2', 'Rainfall_lag_3']\n",
      "Rainfall roll cols found: ['Rainfall_roll12', 'Rainfall_roll3']\n",
      "Temperature lag cols found: ['Temperature_lag_1', 'Temperature_lag_12', 'Temperature_lag_2', 'Temperature_lag_3']\n",
      "Temperature roll cols found: ['Temperature_roll12', 'Temperature_roll3']\n",
      "Saved per-region models for Central -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_Central.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_Central.joblib\n",
      "Saved per-region models for Central -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_Central.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_Central.joblib\n",
      "Saved per-region models for East -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_East.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_East.joblib\n",
      "Saved per-region models for East -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_East.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_East.joblib\n",
      "Saved per-region models for North -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_North.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_North.joblib\n",
      "Saved per-region models for North -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_North.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_North.joblib\n",
      "Saved per-region models for South -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_South.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_South.joblib\n",
      "Saved per-region models for South -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_South.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_South.joblib\n",
      "Saved per-region models for West -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_West.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_West.joblib\n",
      "Saved per-region models for West -> ..\\4_data_analysis\\model_datasets\\models\\rf_rain_West.joblib ..\\4_data_analysis\\model_datasets\\models\\rf_temp_West.joblib\n",
      "Saved forecasts to ..\\4_data_analysis\\model_datasets\\final_forecasts_to_2030.csv\n",
      "Saved forecasts to ..\\4_data_analysis\\model_datasets\\final_forecasts_to_2030.csv\n"
     ]
    }
   ],
   "source": [
    "# Forecast to 2030 (per-region recursive forecasting)\n",
    "from datetime import datetime\n",
    "\n",
    "df_full = df.copy()\n",
    "# Ensure Time exists and rows are ordered per region\n",
    "if 'Time' not in df_full.columns:\n",
    "    df_full = df_full.sort_values(['REGION','YEAR','Month_Num']).reset_index(drop=True)\n",
    "    df_full['Time'] = df_full.groupby('REGION').cumcount()\n",
    "\n",
    "def find_feature_groups(cols, prefix):\n",
    "    # Finds columns containing prefix and \"lag\" or \"roll\"\n",
    "    lag_cols = [c for c in cols if re.search(rf'^{prefix}.*lag', c, flags=re.I)]\n",
    "    roll_cols = [c for c in cols if re.search(rf'^{prefix}.*roll|{prefix}.*rolling', c, flags=re.I)]\n",
    "    return sorted(lag_cols), sorted(roll_cols)\n",
    "\n",
    "all_cols = df_full.columns.tolist()\n",
    "rain_lags, rain_rolls = find_feature_groups(all_cols, 'Rainfall')\n",
    "temp_lags, temp_rolls = find_feature_groups(all_cols, 'Temperature')\n",
    "\n",
    "exclude = ['YEAR','Month','Month_Num','Rainfall','Temperature','Time','REGION']\n",
    "base_features = [c for c in df_full.columns if c not in exclude and c not in [*rain_lags, *rain_rolls, *temp_lags, *temp_rolls]]\n",
    "print('Base features (kept static):', base_features)\n",
    "print('Rainfall lag cols found:', rain_lags)\n",
    "print('Rainfall roll cols found:', rain_rolls)\n",
    "print('Temperature lag cols found:', temp_lags)\n",
    "print('Temperature roll cols found:', temp_rolls)\n",
    "\n",
    "def next_month(year, month):\n",
    "    month += 1\n",
    "    if month > 12:\n",
    "        month = 1\n",
    "        year += 1\n",
    "    return year, month\n",
    "\n",
    "end_year, end_month = 2030, 12\n",
    "\n",
    "final_rows = []\n",
    "for region, g in df_full.groupby('REGION'):\n",
    "    g = g.sort_values(['YEAR','Month_Num']).reset_index(drop=True)\n",
    "    # Build feature_cols list for this region based on available columns\n",
    "    feature_cols = base_features + rain_lags + rain_rolls + temp_lags + temp_rolls\n",
    "    feature_cols = [c for c in feature_cols if c in g.columns]\n",
    "\n",
    "    # Train per-region models on full history\n",
    "    X_full = g[feature_cols]\n",
    "    y_rain = g['Rainfall'].values\n",
    "    y_temp = g['Temperature'].values\n",
    "\n",
    "    rf_rain_reg = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf_temp_reg = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf_rain_reg.fit(X_full, y_rain)\n",
    "    rf_temp_reg.fit(X_full, y_temp)\n",
    "\n",
    "    # Save per-region models\n",
    "    rn = re.sub(r'\\W+','_', region)\n",
    "    prain_path = os.path.join(models_dir, f'rf_rain_{rn}.joblib')\n",
    "    ptemp_path = os.path.join(models_dir, f'rf_temp_{rn}.joblib')\n",
    "    joblib.dump(rf_rain_reg, prain_path)\n",
    "    joblib.dump(rf_temp_reg, ptemp_path)\n",
    "    print(f'Saved per-region models for {region} ->', prain_path, ptemp_path)\n",
    "\n",
    "    # Prepare history for recursive forecasting (use actuals + future predicted appended)\n",
    "    history = g.copy()\n",
    "    last_year, last_month = int(history.iloc[-1]['YEAR']), int(history.iloc[-1]['Month_Num'])\n",
    "    y, m = next_month(last_year, last_month)\n",
    "\n",
    "    future_dates = []\n",
    "    while (y < end_year) or (y == end_year and m <= end_month):\n",
    "        future_dates.append((y, m))\n",
    "        y, m = next_month(y, m)\n",
    "\n",
    "    for (fy, fm) in future_dates:\n",
    "        new_row = {}\n",
    "        new_row['YEAR'] = fy\n",
    "        new_row['Month_Num'] = fm\n",
    "        new_row['Time'] = history['Time'].iloc[-1] + 1\n",
    "\n",
    "        # static base features: carry forward last known value or compute simple time features\n",
    "        for bf in base_features:\n",
    "            if bf in history.columns:\n",
    "                new_row[bf] = history[bf].iloc[-1]\n",
    "            else:\n",
    "                if bf == 'Month_sin':\n",
    "                    new_row['Month_sin'] = np.sin(2*np.pi*(fm/12))\n",
    "                elif bf == 'Month_cos':\n",
    "                    new_row['Month_cos'] = np.cos(2*np.pi*(fm/12))\n",
    "                elif bf == 'YEAR':\n",
    "                    new_row['YEAR'] = fy\n",
    "                else:\n",
    "                    new_row[bf] = np.nan\n",
    "\n",
    "        # compute Rainfall lag features\n",
    "        for lag_col in rain_lags:\n",
    "            m_l = re.search(r'lag[_\\-]?(\\d+)', lag_col, flags=re.I)\n",
    "            if m_l:\n",
    "                k = int(m_l.group(1))\n",
    "                if len(history) >= k:\n",
    "                    new_row[lag_col] = history['Rainfall'].iloc[-k]\n",
    "                else:\n",
    "                    new_row[lag_col] = history['Rainfall'].iloc[0]\n",
    "            else:\n",
    "                new_row[lag_col] = history[lag_col].iloc[-1] if lag_col in history.columns else np.nan\n",
    "\n",
    "        # compute Temperature lag features\n",
    "        for lag_col in temp_lags:\n",
    "            m_l = re.search(r'lag[_\\-]?(\\d+)', lag_col, flags=re.I)\n",
    "            if m_l:\n",
    "                k = int(m_l.group(1))\n",
    "                if len(history) >= k:\n",
    "                    new_row[lag_col] = history['Temperature'].iloc[-k]\n",
    "                else:\n",
    "                    new_row[lag_col] = history['Temperature'].iloc[0]\n",
    "            else:\n",
    "                new_row[lag_col] = history[lag_col].iloc[-1] if lag_col in history.columns else np.nan\n",
    "\n",
    "        # rolling features for Rainfall\n",
    "        for roll_col in rain_rolls:\n",
    "            mm = re.search(r'roll[_\\-]?(\\d+)', roll_col, flags=re.I)\n",
    "            if mm:\n",
    "                w = int(mm.group(1))\n",
    "                vals = history['Rainfall'].iloc[-w:] if len(history) >= w else history['Rainfall']\n",
    "                new_row[roll_col] = vals.mean() if len(vals)>0 else history['Rainfall'].iloc[-1]\n",
    "            else:\n",
    "                new_row[roll_col] = history[roll_col].iloc[-1] if roll_col in history.columns else np.nan\n",
    "\n",
    "        # rolling features for Temperature\n",
    "        for roll_col in temp_rolls:\n",
    "            mm = re.search(r'roll[_\\-]?(\\d+)', roll_col, flags=re.I)\n",
    "            if mm:\n",
    "                w = int(mm.group(1))\n",
    "                vals = history['Temperature'].iloc[-w:] if len(history) >= w else history['Temperature']\n",
    "                new_row[roll_col] = vals.mean() if len(vals)>0 else history['Temperature'].iloc[-1]\n",
    "            else:\n",
    "                new_row[roll_col] = history[roll_col].iloc[-1] if roll_col in history.columns else np.nan\n",
    "\n",
    "        # Build X row for prediction\n",
    "        X_row = []\n",
    "        for col in feature_cols:\n",
    "            X_row.append(new_row.get(col, np.nan))\n",
    "        X_row_df = pd.DataFrame([X_row], columns=feature_cols)\n",
    "\n",
    "        # Predict\n",
    "        p_r = rf_rain_reg.predict(X_row_df)[0]\n",
    "        p_t = rf_temp_reg.predict(X_row_df)[0]\n",
    "        p_r = max(0.0, p_r)\n",
    "\n",
    "        new_row['Rainfall'] = p_r\n",
    "        new_row['Temperature'] = p_t\n",
    "\n",
    "        if 'Month_sin' in feature_cols and 'Month_sin' not in new_row:\n",
    "            new_row['Month_sin'] = np.sin(2*np.pi*(fm/12))\n",
    "        if 'Month_cos' in feature_cols and 'Month_cos' not in new_row:\n",
    "            new_row['Month_cos'] = np.cos(2*np.pi*(fm/12))\n",
    "\n",
    "        # Append new_row to history\n",
    "        history = pd.concat([history, pd.DataFrame([new_row])], ignore_index=True, sort=False)\n",
    "\n",
    "        # Save final row to results\n",
    "        date = pd.Timestamp(year=int(fy), month=int(fm), day=1)\n",
    "        final_rows.append({\n",
    "            'DATE': date,\n",
    "            'REGION': region,\n",
    "            'Predicted_Rainfall': p_r,\n",
    "            'Predicted_Temperature': p_t\n",
    "        })\n",
    "\n",
    "# Save final forecasts\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "final_df = pd.DataFrame(final_rows)\n",
    "final_df = final_df.sort_values(['REGION','DATE']).reset_index(drop=True)\n",
    "out_path = os.path.join(base_dir, 'final_forecasts_to_2030.csv')\n",
    "final_df.to_csv(out_path, index=False)\n",
    "print('Saved forecasts to', out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
